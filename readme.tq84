vi: foldmethod=marker foldmarker={{{,}}}

loadModel (index.html)             Note: there's a function loadModel in index.html AND in model.             Note: there's a function loadModel in index.html AND in model.js
  -> initialize (model.js)
     -> loadModel (model.js)

        -> loadParameters()
           Reads params_gpt.json
           and calculates some other values from it

        -> loadEmbeddings()
           Fetches transformer.wte.weight_gpt.bin
           returns { embeddingsBuffers, deEmbeddingsBuffers}

        -> loadPositionalEmbeddings()
           Fetches transformer.wpe.weight_gpt.bin

        -> loadLayers()
        -> loadFinalLayerNorm()

        loadModel returns
          {
             output: {
               layer_buffers:       {},
               embeddingsBuffers:   {},
               deEmbeddingsBuffers: {},
               posEmbdBuffer:       {},
               normGammaBuffer:     {},
               normBetaBuffer       {}
             }
             params: {
             }
          }
         
        -> loadLayers (model.js)
            -> loadLayer (model.js)
               -> fetchAndInitTensor (model.js)

     -> assign tokenizer (new GPT2Tokenizer or new SimpleTokenizer)    ( See https://github.com/latitudegames/GPT-3-Encoder for GPT2Tokenizer)

------------------------------

  fetchBin()
     async function fetchBin(url) {
       const response = await fetch(url);
       const buffer = await response.arrayBuffer();
       return new Float32Array(buffer);
     }

-----------------------------

Tokenizer {{{

  
GPT2Tokenizer
  Interesting methods:
     - getVocabSize()
     - load()               
        bpe_file <- weights/tokenization/vocab.bpe
        encoder  <- weights/tokenization/gpt_tokens.json


   vocab.bpe has
    50000 interesting lines
    + 256 byte base tokens
    +   1 special end-of-text token
   -----
    50257 = vocab size 


   The method bpe(token) takes a string and returns a string

   {{{ In the console


       tk84 = new GPT2Tokenizer();
       tk84.load();


       GPTModel.tokenizer.encode('$'   ); //  ->   3 (see gpt_tokens.json)
       tk84              .encode('$'   ); //  ->   3 (see gpt_tokens.json)
       GPTModel.tokenizer.encode('ly'  ); //  -> 306
       GPTModel.tokenizer.encode('roly'); //  -> 305, 306
       tk84.encode("\u0120");             //  -> [undefined, 254]

       tk84.vocab_size; // -> 50257

       tk84.encode("hello world. Hello World. hello,world.")
       --> [31373, 995, 13, 18435, 2159, 13, 23748, 11, 6894, 13]
       tk84.decode([31373, 995, 13, 18435, 2159, 13, 23748, 11, 6894, 13])
       --> hello world. Hello World. hello,world.

       tk84.encode('1234abcd 1234 abcd');
       -> [1065, 2682, 397, 10210, 1105, 2682, 450, 10210]
       tk84.encode('1234abcd   1234 abcd');
       -> [1065, 2682, 397, 10210, 220, 220, 1105, 2682, 450, 10210]


       tk.bpe_ranks['Ġ,f']; // -> 21 (See line 21 in vocab.bpe)


   }}}

   {{{ Bytes, Unicode tec

        0: Ā U+0100 Latin Extended-A
        1: ā U+0101
        2: Ă U+0102
        3: ă U+0103
        4: Ą U+0104
        5: ą U+0105
        6: Ć U+0106
        7: ć U+0107
        8: Ĉ U+0108
        9: ĉ U+0109
       10: Ċ U+010A
       11: ċ U+010B
       12: Č U+010C
       13: č U+010D
       14: Ď U+010E
       15: ď U+010F
       16: Đ U+0110
       17: đ U+0111
       18: Ē U+0112
       19: ē U+0113
       20: Ĕ U+0114
       21: ĕ U+0115
       22: Ė U+0116
       23: ė U+0117
       24: Ę U+0118
       25: ę U+0119
       26: Ě U+011A
       27: ě U+011B
       28: Ĝ U+011C
       29: ĝ U+011D
       30: Ğ U+011E
       31: ğ U+011F
       32: Ġ U+0120

       33: ! U+0021 Basic Latin
       34: " U+0022
       35: # U+0023
       36: $ U+0024
       37: % U+0025
       38: & U+0026
       39: ' U+0027
       40: ( U+0028
       41: ) U+0029
       42: * U+002A
       43: + U+002B
       44: , U+002C
       45: - U+002D
       46: . U+002E
       47: / U+002F
       48: 0 U+0030
       49: 1 U+0031
       50: 2 U+0032
       51: 3 U+0033
       52: 4 U+0034
       53: 5 U+0035
       54: 6 U+0036
       55: 7 U+0037
       56: 8 U+0038
       57: 9 U+0039
       58: : U+003A
       59: ; U+003B
       60: < U+003C
       61: = U+003D
       62: > U+003E
       63: ? U+003F
       64: @ U+0040
       65: A U+0041
       66: B U+0042
       67: C U+0043
       68: D U+0044
       69: E U+0045
       70: F U+0046
       71: G U+0047
       72: H U+0048
       73: I U+0049
       74: J U+004A
       75: K U+004B
       76: L U+004C
       77: M U+004D
       78: N U+004E
       79: O U+004F
       80: P U+0050
       81: Q U+0051
       82: R U+0052
       83: S U+0053
       84: T U+0054
       85: U U+0055
       86: V U+0056
       87: W U+0057
       88: X U+0058
       89: Y U+0059
       90: Z U+005A
       91: [ U+005B
       92: \ U+005C
       93: ] U+005D
       94: ^ U+005E
       95: _ U+005F
       96: ` U+0060
       97: a U+0061
       98: b U+0062
       99: c U+0063
      100: d U+0064
      101: e U+0065
      102: f U+0066
      103: g U+0067
      104: h U+0068
      105: i U+0069
      106: j U+006A
      107: k U+006B
      108: l U+006C
      109: m U+006D
      110: n U+006E
      111: o U+006F
      112: p U+0070
      113: q U+0071
      114: r U+0072
      115: s U+0073
      116: t U+0074
      117: u U+0075
      118: v U+0076
      119: w U+0077
      120: x U+0078
      121: y U+0079
      122: z U+007A
      123: { U+007B
      124: | U+007C
      125: } U+007D
      126: ~ U+007E

      127: ġ U+0121 Latin Extended-A
      128: Ģ U+0122
      129: ģ U+0123
      130: Ĥ U+0124
      131: ĥ U+0125
      132: Ħ U+0126
      133: ħ U+0127
      134: Ĩ U+0128
      135: ĩ U+0129
      136: Ī U+012A
      137: ī U+012B
      138: Ĭ U+012C
      139: ĭ U+012D
      140: Į U+012E
      141: į U+012F
      142: İ U+0130
      143: ı U+0131
      144: Ĳ U+0132
      145: ĳ U+0133
      146: Ĵ U+0134
      147: ĵ U+0135
      148: Ķ U+0136
      149: ķ U+0137
      150: ĸ U+0138
      151: Ĺ U+0139
      152: ĺ U+013A
      153: Ļ U+013B
      154: ļ U+013C
      155: Ľ U+013D
      156: ľ U+013E
      157: Ŀ U+013F
      158: ŀ U+0140
      159: Ł U+0141
      160: ł U+0142

      161: ¡ U+00A1 Latin-1 Supplement
      162: ¢ U+00A2
      163: £ U+00A3
      164: ¤ U+00A4
      165: ¥ U+00A5
      166: ¦ U+00A6
      167: § U+00A7
      168: ¨ U+00A8
      169: © U+00A9
      170: ª U+00AA
      171: « U+00AB
      172: ¬ U+00AC
      173: Ń U+0143  ??? U+00AD is a soft hyphen («SHY»)
      174: ® U+00AE
      175: ¯ U+00AF
      176: ° U+00B0
      177: ± U+00B1
      178: ² U+00B2
      179: ³ U+00B3
      180: ´ U+00B4
      181: µ U+00B5
      182: ¶ U+00B6
      183: · U+00B7
      184: ¸ U+00B8
      185: ¹ U+00B9
      186: º U+00BA
      187: » U+00BB
      188: ¼ U+00BC
      189: ½ U+00BD
      190: ¾ U+00BE
      191: ¿ U+00BF
      192: À U+00C0
      193: Á U+00C1
      194: Â U+00C2
      195: Ã U+00C3
      196: Ä U+00C4
      197: Å U+00C5
      198: Æ U+00C6
      199: Ç U+00C7
      200: È U+00C8
      201: É U+00C9
      202: Ê U+00CA
      203: Ë U+00CB
      204: Ì U+00CC
      205: Í U+00CD
      206: Î U+00CE
      207: Ï U+00CF
      208: Ð U+00D0
      209: Ñ U+00D1
      210: Ò U+00D2
      211: Ó U+00D3
      212: Ô U+00D4
      213: Õ U+00D5
      214: Ö U+00D6
      215: × U+00D7
      216: Ø U+00D8
      217: Ù U+00D9
      218: Ú U+00DA
      219: Û U+00DB
      220: Ü U+00DC
      221: Ý U+00DD
      222: Þ U+00DE
      223: ß U+00DF
      224: à U+00E0
      225: á U+00E1
      226: â U+00E2
      227: ã U+00E3
      228: ä U+00E4
      229: å U+00E5
      230: æ U+00E6
      231: ç U+00E7
      232: è U+00E8
      233: é U+00E9
      234: ê U+00EA
      235: ë U+00EB
      236: ì U+00EC
      237: í U+00ED
      238: î U+00EE
      239: ï U+00EF
      240: ð U+00F0
      241: ñ U+00F1
      242: ò U+00F2
      243: ó U+00F3
      244: ô U+00F4
      245: õ U+00F5
      246: ö U+00F6
      247: ÷ U+00F7
      248: ø U+00F8
      249: ù U+00F9
      250: ú U+00FA
      251: û U+00FB
      252: ü U+00FC
      253: ý U+00FD
      254: þ U+00FE
      255: ÿ U+00FF

   }}}

   {{{ Regexp /'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+/gu

     Purpose:
         This regular expression is designed to split input text into meaningful tokens for the GPT-2 model, handling various types of input like:

       's|'t|'re|'ve|'m|'ll|'d:

          Matches common English contractions like:
             's (e.g., "it's", "he's")
             't (e.g., "don't", "can't")
             're (e.g., "they're")
             've (e.g., "we've")
             'm (e.g., "I'm")
             'll (e.g., "she'll", "I'll")
             'd (e.g., "he'd", "you'd")
                 This part ensures that contractions are treated as separate tokens.

        ?\p{L}+:

          \p{L} matches any kind of letter from any language (when used with /u)
          + ensures it matches one or more letters.
 
          The preceding ? means it optionally matches a space before the letters, which ensures that words are tokenized properly while keeping any leading spaces as part of the token (if needed).

       ?\p{N}+:

           \p{N} matches any numeric character
          + ensures it matches one or more digits.
          The optional leading space ( ?) handles numbers in the same way as letters.

      ?[^\s\p{L}\p{N}]+:

          ^\s\p{L}\p{N} matches anything that is not a whitespace character (\s), a letter (\p{L}), or a number (\p{N}).

          This part is for handling punctuation, symbols, or other non-alphanumeric characters.

          The optional leading space ensures any punctuation or special characters that appear with spaces (e.g., " ,") are handled properly.

      \s+(?!\S):

         \s+ matches one or more whitespace characters.

         (?!\S) is a negative lookahead that ensures the whitespace is followed by nothing (i.e., end of input) or more whitespace.
         This part likely handles trailing spaces or empty spaces, preventing excessive whitespace from being tokenized as a separate token.

      \s+:

          Matches one or more spaces, ensuring spaces are tokenized separately when necessary.

      Flags:
         - g: Global flag, meaning it will search for all matches in the input, not just the first one.
         - u: Unicode flag, enabling support for Unicode in the regex (this is crucial for \p{L} and \p{N} to work properly).


    }}}


}}}
-----------------------------

Visuals
  The canvas seems to display n_ebmd * n_ctx dots

-----------------------------

Tokenizer type BPE = Byte Pair Encoding

this project knows two tokenizer types:
  - char (only used for the shakespeare model)
  - bpe

Operations (see globals.js)
  const FastMatMulBlock = new FastMatMulBlockClass();
  const AttentionBlock  = new AttentionBlockClass();
  const ResidualBlock   = new ResidualBlockClass();
  const EmbedBlock      = new EmbedBlockClass();
  const DeEmbedBlock    = new DeEmbedBlockClass();
  const GeluBlock       = new GeluBlockClass();
  const LayerNormBlock  = new LayerNormBlockClass();
  const SoftmaxBlock    = new SoftmaxBlockClass();

Some relationsships of parameters in loadParameters() (model.js)

What is minBufferOffset (=1) used for?


{ Files in the weights/xyz folder

  transformer.wte.weight_gpt.bin          (loaded in loadEmbeddings() into variable embeddingsWeights )  - wte = Word token embeddings  - size = 38597376 | used to map each token in the input sequence to a high-dimensional vector representatio

}

{ Function transpose (globals.js)

  Translate from row major to column major and vice versa

  transpose([0,2,4 , 1,3,5 ], 2, 3)
  => Float32Array(6) [0, 1, 2, 3, 4, 5]

  transpose([0,1 , 2,3 , 4,5], 3, 2)
  => Float32Array(6) [0, 2, 4, 1, 3, 5]


}
{{{ Function initTensor

  calls device.createBuffer

  The third parameter (ops) is an array of strings which indicate
  members of the GPUBufferUsage enum (see bufferUsageDict in globals.js)

}}}
{{{ Links

   GPT2 From Scratch In C++ - Part 1 - Tokenization    -> https://www.daoplays.org/blog/gpt2_p1 

}}}
